---
title: 'Alignment, Simplified: Steering LLMs with Self-Generated Preferences'
date: 2025-02-27
permalink: /posts/2025/02/alignez/
tags:
  - Self-Alignment
  - Inference-time steering
excerpt: ''
authors: "<a href='https://dyahadila.github.io/'>Dyah Adila</a>" 
---

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']]
  }
};
</script>

Imagine you can steer a language model's behavior on the fly- no extra training, no rounds of fine-tuning, just on-demand alignment. In our paper, **"Alignment, Simplified: Steering LLMs with Self-Generated Preferences"**, we show that this isn't just possible—it’s practical, even in complex scenarios like pluralistic alignment and personalization.

## The Traditional Alignment Bottleneck

Traditional LLM alignment requires two critical components: (1) collecting large volumes of preference data, and (2) using this data to further optimize pretrained model weights to better follow these preferences. As models continue to scale, these requirements become increasingly prohibitive—creating a bottleneck in the deployment pipeline.

This problem intensifies when facing the growing need to align LLMs to multiple, often conflicting preferences simultaneously [(Sorensen et al., 2024)](https://arxiv.org/abs/2402.05070), alongside mounting demands for rapid, fine-grained individual user preference adaptation [(Salemi et al., 2023)](https://maroo.cs.umass.edu/getpdf.php?id=1480). 

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/alignez/shocked-surprised.gif" width="200">
</p>

Our research challenges this status quo: Must we always rely on expensive data collection and lengthy training cycles to achieve effective alignment?

The evidence suggests we don't. When time and resources are limited—making it impractical to collect large annotated datasets—traditional methods like DPO struggle significantly with few training samples. Our more cost-effective approach, however, consistently outperforms these conventional techniques across multiple benchmarks, as demonstrated below:

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/alignez/time_sensitive_exp.png">
</p>

These results reveal a clear path forward: alignment doesn't have to be a resource-intensive bottleneck in your LLM deployment pipeline. Enter AlignEZ—our novel approach that reimagines how models can adapt to preferences without the traditional overhead.

---

## On the fly Alignment: The EZ Solution

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/alignez/alignez_main.jpg">
</p>

At its core, **AlignEZ** enables the (non-trivial) combination of two most cost-efficient choice of data and algorithm--using self-generated preference data and cut down the compute cost by replacing fine-tuning with embedding editing. This combination is non-trivial for several reasons: 
- Model generated signal is often noisier in nature, necessitating an approach that is able to effectively harness alignment signal from the noise.
- On top of assuming access to clean human-labeled data, current embedding editing approaches assumes 1-vector-fits-all for to steer the LLM embeddings. By operating in carefully identified subspaces, it enables seamless extension to multi-objective alignment scenarios.

Now that hopefully have convinced you why this is the way to go, let's break down how AlignEZ works, in plain English:

### Step 1: Self-generated Preference Data

Instead of collecting human-labeled preference data, AlignEZ lets the model create its own **diverse** preference pairs. Diversity is key to capturing a broad range of alignment signals, ensuring we capture as much alignment signal as possible. We achieve this through a two-step prompting strategy:

1. For each test query, we prompt the model to explicitly identify characteristics that distinguish "helpful" from "harmful" responses. This process is customized to the specific task at hand. For example, in a writing task, we might contrast attributes like "creative" versus "dull" to tailor the alignment signals appropriately.
3. The model then generates multiple responses to the test query, each deliberately conditioned on the identified characteristics.

By applying this process across our dataset, we develop a rich preference dataset where each query is paired with multiple responses that reflect various dimensions of "helpful" and "unhelpful" behavior.

Importantly, we recognize that the initial batch of generated data may contain significant noise—often resulting from the model failing to properly follow the conditioned characteristic. As a critical first filtering step, we eliminate samples that are too similar in the embedding space, a characteristic that research by [(Razin et al., 2024)](https://arxiv.org/abs/2410.08847) has shown to increase the likelihood of dispreferred responses.

### Step 2: Identify Alignment Subspace

With our self-generated preference data in hand, we next identify the alignment subspace within the LLM's latent representation. Our approach adapts classic techniques from embedding debiasing literature (Bolukbasi et al., 2016) that were originally developed to identify subspaces representing specific word groups. 

Formally, let $\Phi_l$ denote the function mapping an input sentence to the embedding space at layer $l$, and each preference pair as $(p_i^{help}, p_i^{harm})$. Firt, we construct embedding matrices for helpful and harmful preferences:  

$$ 
\begin{equation}

\textbf{H}_{l}^{help} := 
\begin{bmatrix}
\Phi_{l}(p_1^{help}) \\ 
\vdots \\ 
\Phi_{l}(p_K^{help})
\end{bmatrix}^T, 
\quad 
\textbf{H}_{l}^{harm} := 
\begin{bmatrix}
\Phi_{l}(p_1^{harm}) \\ 
\vdots \\ 
\Phi_{l}(p_K^{harm})
\end{bmatrix}^T,
\end{equation}
$$

where $K$ is the total number of preference pairs. Next, alignment subspace is identified by computing the difference between the helpful and harmful embeddings:  

$$
\begin{equation}
    \textbf{H}_{l}^{align} := \textbf{H}_{l}^{help} - \textbf{H}_{l}^{harm}.
\end{equation}
$$

We then perform SVD on $\textbf{H}_{l}^{align}$:  
$$
\begin{equation}
     \textbf{H}_{l}^{align} = \textbf{U}\Sigma\textbf{V} \\
     \Theta_l^{align} := \textbf{V}^T,
\end{equation}
$$

An important trick we add here is to remove subspace directions that are already well-represented in the original LLM embedding. Formally for a query $q$:

$$
\begin{equation}
\Theta_{l,help}^{align}(q) := \left\{\,\theta \in \Theta_l^{align} \,\middle|\,
 \cos\left(\Phi_l(q),\theta\right) \leq 0 \right\}, 
\end{equation}
$$

This prevents any single direction from dominating the editing process and ensures we only add necessary new directions to the embedding space.

### Step 3: Edit Embeddings During Inference

Finally, during inference when generating a new response, we modify the model's hidden representations by projecting them in the direction of the alignment subspace $\Theta_l^{align}$. Our editing process is as follow:

$$
\begin{aligned}
\hat{x}_l &\leftarrow x_l,\\
\text{for each } \theta_l \in \Theta_l^{align}:\quad 
\hat{x}_l &\leftarrow \hat{x}_l 
+ \alpha\,\sigma\!\bigl(\langle \hat{x}_l, \theta_l \rangle\bigr)\,\theta_l,
\end{aligned}
$$

where $\sigma(\cdot)$ is an activation function and $\langle \cdot,\cdot \rangle$ denotes inner product. We iteratively adjust $\hat{x}_l$ by moving it toward or away from each direction $\theta_l$ in $\Theta_l$. For harmful subspace removal, we set $\sigma(\cdot)=\mathrm{ReLU}(\cdot)$ with $\alpha = -1$, ensuring subtraction only when $\hat{x}_l$ aligns with harmful directions. For helpful subspaces, we set $\sigma(\cdot)=\tanh(\cdot)$ with $\alpha = 1$, enabling smooth bidirectional scaling bounded by $[-1,1]$.

### Why It Works: The Intuition

The core insight behind AlignEZ is that **alignment information already exists within the pre-trained model** - we just need to find it and amplify it.

Think of it like a radio signal. The alignment "station" is already broadcasting inside the model, but it's mixed with static. Traditional methods try to boost the signal by retraining the entire radio (expensive!). AlignEZ instead acts like a targeted equalizer that simply turns up the volume on the channels where the alignment signal is strongest.

For more detailed explanation of our method with proper mathematical notations, refer to [our paper](https://arxiv.org/pdf/2406.03642).

---
<!-- 

## Results That Speak Volumes

The experiments reported in the paper are nothing short of impressive:

- **Alignment Boosts:** ALIGNEZ improves alignment performance by up to **19.9%** on general tasks and even shows gains (around **1.9%**) on challenging mathematical reasoning tasks.
- **Multi-Objective Control:** Not only can it steer models towards being more helpful or harmless, but it can also manage multiple objectives simultaneously. This multi-dimensional control is a big win for applications requiring nuanced behavior.
- **Accelerated Alignment:** When used to enhance methods like DPO—especially with scarce ground-truth data—ALIGNEZ brings performance up to par with models trained on much larger datasets.

These results demonstrate that, sometimes, the best alignment strategy might be to let the model help itself, a philosophy that feels both intuitively elegant and practically promising.

---

## Implications and Future Directions

The ALIGNEZ approach opens up a range of exciting possibilities:

- **Rapid Personalization:** In settings where time and computational resources are limited, on-the-fly alignment means LLMs can be customized for diverse user needs without heavy overhead.
- **Resource Efficiency:** By sidestepping the need for extensive human annotation and fine-tuning, ALIGNEZ lowers the barrier for deploying aligned models at scale.
- **Broader Applications:** Beyond safety and ethical alignment, the same principles could be used to boost specialized capabilities, like advanced reasoning or domain-specific expertise.

In essence, the paper paves the way for more agile, responsive, and cost-effective approaches to LLM alignment.

---

## Final Thoughts

"Alignment, Simplified" is a refreshing take on a long-standing challenge in AI research. By harnessing the model's own internal capabilities to generate preference data and edit its representations, the authors not only challenge conventional wisdom but also chart a course toward more efficient, multi-objective, and scalable alignment strategies.

For anyone interested in the cutting edge of model alignment—and who appreciates a touch of engineering elegance reminiscent of [Roboshot](https://sprocketlab.github.io/posts/2023/07/roboshot/)—this paper is a must-read. It’s a vivid reminder that sometimes, the best innovations come from rethinking the basics and trusting the system’s innate strengths.

---

*Source: "Alignment, Simplified: Steering LLMs with Self-Generated Preferences" :contentReference[oaicite:0]{index=0}* --> --> -->
