---
title: 'Weak-to-Strong Generalization Through a Data-Centric Lens'
date: 2025-02-25
permalink: /posts/2025/02/weak-to-strong-generalization/
tags:
  - Weak-to-Strong Generalization
  - Data-Centric AI
  - Overlap Density
  - Data Source Selection
excerpt: 'Exploring how overlap density drives weak-to-strong generalization and its applications in data source selection.'
authors: "<a href='https://ch-shin.github.io'>Changho Shin</a>"
---

In recent years, the phenomenon of **weak-to-strong generalization** has captured significant attention. This setting involves training a strong model using labels or supervisory signals generated by a weak model—yet, remarkably, the strong model eventually outperforms the weak model. Our work takes a **data-centric** perspective, exploring how the intrinsic properties of data drive this improvement.

## What is Weak-to-Strong Generalization?

<figure align="center">
  <img src="https://sprocketlab.github.io/images/blogposts/w2s/w2s.jpg" alt="W2S">
  <figcaption>Weak-to-Strong Generalization. </figcaption>
</figure>


Weak-to-strong generalization describes a scenario in which a strong model, trained on pseudolabels or outputs provided by a weak model, achieves superior performance compared to the weak model itself. In such settings, the weak model is capable of making predictions on a broad range of data, while the strong model leverages these predictions as a foundation to learn additional, more complex aspects of the data. This phenomenon is critical in contexts such as data-efficient learning and has implications for building more advanced, robust systems.


## Data-Centric View: Overlap Density

<figure align="center">
  <img src="https://sprocketlab.github.io/images/blogposts/w2s/overlap_density.png" alt="Overlap Density">
  <figcaption>Adding more such overlapping points has little influence on the performance of the weak model, but dramatically improves the performance of the weak-to-strong model. Adding such points---even a small percentage of the dataset---can push against the limits of the strong model. </figcaption>
</figure>

A key insight from our work is that the potential for weak-to-strong generalization is driven by the **overlap density** in the data. In our framework, each data point can be viewed as containing two types of informative patterns: one that the weak model can readily capture and another that is more challenging and requires the capacity of a strong model. The **overlap points** are those that contain both types of patterns, and their density in the dataset—the overlap density—is a critical factor in enabling the strong model to learn effectively.

### Overlap Detection in Practice

Because overlap density is not directly observable in real-world data, we developed an **overlap detection algorithm** that operates in two main steps:

1. **Confidence-Based Separation:**  
   The algorithm first uses the weak model’s confidence scores to separate data points. Typically, points where the weak model is less confident are likely to lack the patterns it can easily capture. By thresholding these confidence scores (using methods like change-point detection), we identify a subset of points that are likely to be dominated by the challenging patterns.

2. **Overlap Scoring:**  
   For the remaining points, we compute an overlap score that measures the degree to which they exhibit characteristics of both the readily learnable patterns and the more challenging ones. Points with high overlap scores are then classified as overlap points.

<figure align="center">
  <img src="https://sprocketlab.github.io/images/blogposts/w2s/real_exp_overlap_detection.png" alt="Overlap Detection">
  <figcaption> Overlap density versus performance in weak-to-strong generalization with LLMs. Red lines show strong ceiling model accuracies, blue dashed lines represent weak model test accuracies, and W2S lines represent the accuracies of strong models trained on pseudolabeled data with a controlled proportion of overlap density. In general, **the strong model's improvement over the weak model tracks the overlap proportion, suggesting that the overlap density is indeed an important mechanism for generalization**. We can observe three different regimes of weak-to-strong generalization in our experiments: a **low overlap regime**, where the overlap density is insufficient for effective weak-to-strong generalization (here, few points contain overlaps, so choosing to rely on a large overlap proportion translates to a small train set), a **medium overlap regime**, where the overlap density improves generalization but still yields performance close to that of the weak model, and a **high-overlap regime**, where the strong model's performance approaches that of the true strong model due to sufficient overlap points. </figcaption>
</figure>

This procedure yields an estimate of the overlap density in a dataset, which is crucial for understanding and enhancing weak-to-strong generalization.

## Data Source Selection for Maximizing Overlap Density

Beyond estimating overlap density within a single dataset, our approach extends to selecting the best data sources from multiple candidates. The idea is to prioritize sources that exhibit a high overlap density, as these are more likely to provide supervisory signals that enable the strong model to learn the challenging aspects of the data.

Our UCB-based (Upper Confidence Bound) data selection algorithm works as follows:

1. **Initialization:**  
   - Sample a small batch from each available data source.
   - Apply the overlap detection algorithm to estimate the overlap density for each source.

2. **UCB Computation:**  
   - For each source, calculate an upper confidence bound on its overlap density, balancing exploration (sampling less-visited sources) with exploitation (focusing on sources with high estimated overlap).

3. **Iterative Selection:**  
   - Over several rounds, select the data source with the highest UCB score.
   - Sample additional data from that source, update the overlap density estimates, and repeat the process.


<figure align="center">
  <img src="https://sprocketlab.github.io/images/blogposts/w2s/llm_data_selection.png" alt="Data Source Selection">
  <figcaption> Data selection results with UCB-based data selection algorithm for Amazon Polarity and DREAM datasets. We observe that the data source selection procedure, based on overlap density estimation, can produce enhancements over random sampling across data sources. </figcaption>
</figure>


Our experiments on datasets such as Amazon Polarity and DREAM demonstrate that this UCB-based strategy consistently identifies data sources with higher overlap density. When training the strong model with data selected through our algorithm, we observe improved generalization performance compared to using randomly sampled data.

## Concluding Thoughts: A Path to Superintelligence
<figure align="center">
  <img src="https://sprocketlab.github.io/images/blogposts/w2s/path_to_superintelligence.png" alt="Path to Superintelligence">
  <figcaption> Weak-to-Strong Generalization: A Pathway to Superintelligence. </figcaption>
</figure>

Our exploration into weak-to-strong generalization through the lens of overlap density offers a novel, data-centric approach to improving model performance. By identifying and leveraging data points that exhibit both readily learnable and challenging patterns, we can enhance the effectiveness of weak supervision and drive robust generalization. This strategy—prioritizing the right data sources—may prove to be a crucial step on the path toward building truly advanced systems.

Weak-to-strong generalization could be key to achieving superintelligence. As AI continues to advance, the pool of human experts capable of providing meaningful supervision is shrinking. For instance, as mathematicians are recruited to annotate difficult math questions, the availability of expertise becomes a bottleneck. In the long run, efficient supervisory signals will be essential, and understanding weak-to-strong generalization is vital. In many ways, this process mirrors human academia—learning from imperfect past knowledge, generalizing it, and pushing it forward.

To delve further into our theoretical analysis and experimental findings, please refer to our paper at [https://arxiv.org/abs/2412.03881](https://arxiv.org/abs/2412.03881) and explore our implementation on GitHub at [https://github.com/SprocketLab/datacentric_w2s](https://github.com/SprocketLab/datacentric_w2s).