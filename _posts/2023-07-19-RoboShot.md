---
title: 'Zero-Shot Robustification of Zero-Shot Models With Auxiliary Foundation Models'
date: 2023-07-19
permalink: /posts/2023/07/roboshot/
tags:
  - Multi-modal Models
  - Zero-shot inference
  - Robust ML
  - Language Models
excerpt: ''
authors: "<a href='https://dyahadila.github.io/'>Dyah Adila</a> and <a href='https://pages.cs.wisc.edu/~fredsala/'> Fred Sala </a>" 
---
Large pre-trained multi-modal models (e.g., OpenAI’s CLIP) are strong zero-shot predictors–achieving 77.9% zero-shot accuracy on CIFAR100. Now practitioners can use them out of the box for downstream prediction tasks without fine-tuning. Unfortunately, such models can adopt biases or unwanted correlations from their large-scale training data – making their predictions less reliable on samples that break in-distribution correlation. For instance, they might associate 'cow' with 'green'; because cow images are mostly depicted with pastures in the training data. But then, making wrong predictions on images of, let's say, cows on the beach.  Traditionally we can always fine-tune these models to get better performance on all groups in our test data. However, this breaks the promise of large pre-trained models – their capacity to be used out of the box. 

In this post we describe Roboshot: an approach to robustify pre-trained models and steering them away from these biases/correlations. What's more? RoboShot does this **without additional data and fine-tuning**! The core of our idea is inspired by embedding debiasing literature, which seeks to remove subspaces that contain predefined harmful or unwanted concepts. However, here we do not seek to produce fully-invariant embeddings; **our goal is simply to improve pre-trained model robustness** at low or zero cost.


<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/main_block.png">
</p>

# Zero-shot inference and modeling


Before diving in, let's first discuss and formulate the zero-shot inference setup. As has been studied in the literature, we can think of pre-trained models embedding space as spanning unknown concepts ${z_1, z_2, \ldots, z_k}$; then the embedding produced by pre-trained model $x$ is a mixture of concepts $\Sigma_i \gamma_i z_i$, where $\gamma_i \geq 0$ are weights.

Now, we describe the formulation for zero-shot binary classification (it is straightforward to extend to multi-class settings). We take $\Sigma_i \alpha_i z_i$ to be data sample embedding, and $c^0=\sum_i \beta_{i,0} z_i$ is the embedding of the first class, and $c^1=\sum_i \beta_{i,1} z_i$ is that of the second. Zero-shot prediction is made by 

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/zeroshot_pred.png"  width="450">
</p>

We predict the class that has the higher inner product with the datapoint's embedding.

We assume that input embedding mixture can be partitioned into three concept groups: harmful, helpful, and benign

$$  x = \sum_{s=1}^S \alpha_s^{\text{harmful}} z_s + \sum_{r=S+1}^{S+R} \alpha_r^{\text{helpful}} z_r + \sum_{b=S+R+1}^{S+R+B} \alpha_b^{\text{benign}} z_b. $$

For better illustration, we will start with a working example of a benchmark dataset: Watebirds. The task is to distinguish  $y \in \{\texttt{waterbird}, \texttt{landbird}\}$. The training data contains unwanted correlations between waterbird and water background, and landbird with land background. For the sake of illustration, let’s assume that in the embedding space, $z_{water} = -z_{land}$ and $z_{waterbird} = -z_{landbird}$.

Consider a test image of landbird over water, which does not follow the training correlations. In the embedding space, this might be  $x=0.7z_{\texttt{water}}+ 0.3 z_{\texttt{landbird}}$. We may also have class embeddings $c^{\texttt{waterbird}}=0.4z_{\texttt{water}}+0.6z_{\texttt{waterbird}} \text{ and }c^{\texttt{landbird}}=0.4z_{\texttt{land}}+0.6z_{\texttt{landbird}}.$. Then, the zero-shot prediction is $x^T c^{\texttt{waterbird}}= 0.1 > x^T c^{\texttt{landbird}}= -0.1$. This result in waterbird prediction, and thus is incorrect. We have seen how in this example, harmful components contained in $x$ caused wrong predictions. Now, we will demonstrate how Roboshot improves robustness against these unwanted correlations by reducing harmful components in embeddings and boosting the helpful ones.

# RoboShot

Suppose for our sample embedding $x$, we have a ground truth harmful component $v^{\texttt{harmful}}$ and ground truth beneficial component $v^{\texttt{helpful}}$. Note that in reality, we do not have access to the $v^{\texttt{harmful}}$ and $v^{\texttt{helpful}}$. We will describe the proxy for this ground truth component later. Roboshot reduces $v^{\texttt{harmful}}$’s effect on $x$ by classical vector rejection: 

<p align="right">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/v_reject.png"  width="500">
</p>

Intuitively, this procedure subtracts $v^{\texttt{harmful}}$’s component on $x$. Similarly, to increase $v_{\texttt{helpful}}$’s influence, we can add $v^{\texttt{helpful}}$’s component along $x$, such that:

<p align="right">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/u_accept.png"  width="500">
</p>


Let's try this on our example.

Suppose that $v^{\texttt{harmful}}=0.9z_{\texttt{water}}+0.1z_{\texttt{landbird}}$, and that this is our only harmful insight. Similarly, suppose that only have a single helpful insight given by $v^{\text{helpful}}=0.1z_{\texttt{water}}+0.9z_{\texttt{landbird}}$. First, to reduce $v^{\texttt{harmful}}$’s effect, we plug it into equation 2, resulting in $\hat{x} = -0.0244z_{\texttt{water}}+0.2195z_{\texttt{landbird}}.$
Making zero shot prediction with $\hat{x}$, we have that $x^T c^{\texttt{waterbird}}= -0.1415 < x^T c^{\texttt{landbird}}= 0.1415$. By removing a single component, we neutralize the harmful component and thus obtain the correct prediction. Now, let's see the effect of increasing $v^{\texttt{helpful}}$’s effect by plugging it into equation 3. This results in $\hat{x} = -0.0006z_{\texttt{water}}+0.4337z_{\texttt{landbird}}$, which further increase the classification margin. 

<img src="https://sprocketlab.github.io/images/blogposts/roboshot/algorithm.png">

Algorithm 1 details the RoboShot algorithm. In real scenarios, we often have multiple helpful and harmful concepts (e.g., shape of beak, wing size, etc). We can simply do the vector rejection and addition iteratively (lines 2-5 and 6-8, respectively).

# Using LLM to obtain proxy to $v^{\texttt{harmful}}$ an $v^{\texttt{helpful}}$

We have discussed how RoboShot reduces the harmful components and increases helpful ones. Now, how do we get access to $v^{\texttt{harmful}}$ and $v^{\texttt{helpful}}$? especially since in latent space, features are entangled with one another.

Let's take a step back from the latent space and think of $v^{\texttt{harmful}}$ and $v^{\texttt{helpful}}$ in the context of the task. For instance, in the waterbirds dataset, the task is to distinguish between landbird and waterbird, and $v^{\texttt{harmful}}$ is the background (i.e., land or water), and $v^{\texttt{helpful}}$ is the actual bird features (e.g., beak shape, wing size). This way, we can think of $v^{\texttt{harmful}}$  and $v^{\texttt{helpful}}$ as a prior in the task. Now, we have two remaining pieces to tie it all together: (i) how to obtain these insights without training, and (ii) how to translate them in latent space.

In RoboShot, we use **textual** descriptions of harmful and helpful concepts by querying language models (LM) using **only the task description**. For example, in the Waterbirds dataset, we use the prompt *"What are the biased/spurious differences between waterbirds and landbirds?"*. 

We translate the answers we get to $v^{\texttt{harmful}}$ , by using their embeddings. Let $s^1, s^2$ be the text insights obtained from the answer (e.g., {*'water background'* *'land background'*}). We obtain a spurious insight representation by taking the difference of their embedding:

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/v_concept.png"  width="300">
</p>

Similarly, to obtain proxy to $v^{\texttt{helpful}}$, we ask LMs *"What are the true characteristics of waterbirds and landbirds?"* and obtain e.g., {*'short beak'*, *'long beak'*}. The remainder of the procedure is identical to the case of harmful components.

where $g$ is the text encoder of our model. 

# RoboShot in action
 Thats it! With this cheap and finetuning free approach, now we can robustify our zero-shot models against unwanted correlations from training data. In  the table below, we can see that Roboshot improves over zero-shot baselines across multiple spurious correlation and distribution shift benchmarks. We provide more results in our paper.

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/results.png"  width="750">
</p>

We can also see the effect of rejecting $v^{\texttt{harmful}}$, increasing $v^{\texttt{helpful}}$, and doing both in the following image. Rejecting $v^{\texttt{harmful}}$ reduces variance in one direction, while increasing $v^{\texttt{helpful}}$ amplifies variance in the orthogonal direction. When both projections are applied, they create a balanced mixture.

<p align="center">
<img src="https://sprocketlab.github.io/images/blogposts/roboshot/illustration.png"  width="600">
</p>

# Concluding thoughts

In this post, we have described our approach to robustify pre-trained models from unwanted correlations without any fine-tuning. What's interesting about this approach is that it is almost zero-cost: we obtain insights from cheap (or even free) available knowledge resources and use them to improve pre-trained models – defying the usual need to collect extra labels for fine-tuning. Moreover, this approach also opens way to debias multi-modal embeddings – using textual embeddings to debias image embeddings.

Thank you for reading! :blush: Kindly check our :female-technologist: [GitHub repo](https://github.com/SprocketLab/roboshot) and :scroll: paper!
